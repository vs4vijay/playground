# groq: https://api.groq.com/openai/v1
# ollama: http://localhost:11434/api/
# ollama run llama3.2

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "[+] Starting ollama"
        ollama start & sleep 20
        ollama pull phi3 && tail -f /dev/null
    environment:
      - TEST=
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped
    networks:
      - internal
      - web

  # Host on subdomain
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    environment:
      - ENABLE_SIGNUP="False"
      # - OLLAMA_PROXY_URL=
      # - OLLAMA_API_BASE_URL=http://host.docker.internal:11434/api
    ports:
      - "127.0.0.1:9300:8080"
    volumes:
      - open-webui:/app/backend/data
    restart: unless-stopped
    networks:
      - internal
      - web

  # serge:
  #   image: ghcr.io/serge-chat/serge:latest
  #   container_name: serge
  #   # environment:
  #   #   -
  #   ports:
  #     - "127.0.0.1:8008:8008"
  #   volumes:
  #     - weights:/usr/src/app/weights
  #     - datadb:/data/db/
  #   restart: unless-stopped
  #   networks:
  #     - internal
  #     - web

  # lobe-chat:
  #   image: lobehub/lobe-chat:latest
  #   container_name: lobe-chat
  #   # environment:
  #   #   - OLLAMA_PROXY_URL=
  #   ports:
  #     - "127.0.0.1:3210:3210"
  #   restart: unless-stopped
  #   networks:
  #     - internal
  #     - web

volumes:
  ollama:
  open-webui:
  weights:
  datadb:

networks:
  internal:
    name: internal
  web:
    name: web
